{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenderRecognisionTrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemeZamanian/DeepLearning/blob/main/GenderRecognition/GenderRecognisionTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDM6zOIb9Qkv"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE0qAQi9ZBMp"
      },
      "source": [
        "!kaggle datasets download -d ashishjangra27/gender-recognition-200k-images-celeba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ET0edkXaw3e"
      },
      "source": [
        "!unzip -qq gender-recognition-200k-images-celeba.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuLrYZL-cdgU"
      },
      "source": [
        "import os\n",
        "from tensorflow.keras import models,layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qDPm42K4VDn"
      },
      "source": [
        "width=height=224\n",
        "batch_size=32"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiohg_BCd8u_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4ad268-abe4-4bbc-fe81-f047c7c0f1d7"
      },
      "source": [
        "data_generator=ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "train_data=data_generator.flow_from_directory(\n",
        "    \"Dataset/Train\",\n",
        "    target_size=(width,height),\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_data=data_generator.flow_from_directory(\n",
        "    \"Dataset/Validation\",\n",
        "    target_size=(width,height),\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "test_data=data_generator.flow_from_directory(\n",
        "    \"Dataset/Test\",\n",
        "    target_size=(width,height),\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 160000 images belonging to 2 classes.\n",
            "Found 22598 images belonging to 2 classes.\n",
            "Found 20001 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaSmsFHS5IKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4d297d-fb9e-459b-929f-71ba67ed6c86"
      },
      "source": [
        "model_r=tf.keras.applications.ResNet50V2(\n",
        "    input_shape=(width,height,3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "94683136/94668760 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuCtzgF4l4Ae"
      },
      "source": [
        "model_r.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRG8QbRp5zzV"
      },
      "source": [
        "for layer in model_r.layers[:-2]:\n",
        "  layer.trainable=False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlLqyqoj6E4O"
      },
      "source": [
        "model=tf.keras.Sequential([\n",
        "        model_r,\n",
        "        layers.Dense(1024,activation='relu'),\n",
        "        layers.Dense(256,activation='relu'),\n",
        "        layers.Dense(64,activation='relu'),\n",
        "        layers.Dense(32,activation='relu'),\n",
        "        layers.Dense(2,activation='softmax'),\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gfW-PFJ6tsA"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001 ),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'],)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBC2oYZq6yRd",
        "outputId": "0bccea4e-a0ae-473d-da6b-27e7279c289c"
      },
      "source": [
        "model.fit(train_data,\n",
        "          steps_per_epoch=train_data.samples/batch_size,\n",
        "          validation_data=val_data,\n",
        "          validation_steps=val_data.samples/batch_size,\n",
        "          epochs=5,\n",
        "          \n",
        "\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5000/5000 [==============================] - 1129s 219ms/step - loss: 0.1592 - accuracy: 0.9365 - val_loss: 0.1437 - val_accuracy: 0.9446\n",
            "Epoch 2/5\n",
            "5000/5000 [==============================] - 1088s 218ms/step - loss: 0.1313 - accuracy: 0.9480 - val_loss: 0.1392 - val_accuracy: 0.9457\n",
            "Epoch 3/5\n",
            "5000/5000 [==============================] - 1083s 217ms/step - loss: 0.1194 - accuracy: 0.9529 - val_loss: 0.1528 - val_accuracy: 0.9460\n",
            "Epoch 4/5\n",
            "5000/5000 [==============================] - 1089s 218ms/step - loss: 0.1098 - accuracy: 0.9565 - val_loss: 0.1435 - val_accuracy: 0.9478\n",
            "Epoch 5/5\n",
            "5000/5000 [==============================] - 1089s 218ms/step - loss: 0.1018 - accuracy: 0.9600 - val_loss: 0.1350 - val_accuracy: 0.9492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8cfe0d0510>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snvVCPyykYCE"
      },
      "source": [
        "model.save('gender.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0iTsOQCDXdT",
        "outputId": "8b7cccc8-d72b-4b47-e064-5863c27456ee"
      },
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "626/626 [==============================] - 120s 191ms/step - loss: 0.1053 - accuracy: 0.9594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10533639788627625, 0.9593520164489746]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u7A7tSNW5dP",
        "outputId": "56edaee4-dc50-455e-d5ed-76c14684a35c"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "Y_pred = model.predict(test_data)\n",
        "y_pred = np.argmax(Y_pred, axis = 1)\n",
        "print('confusion Matrix')\n",
        "print(confusion_matrix(test_data.classes, y_pred))\n",
        "\n",
        "target_names = list(test_data.class_indices.keys())\n",
        "print('Classification Report')\n",
        "print(classification_report(test_data.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion Matrix\n",
            "[[11070   472]\n",
            " [  346  8113]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Female       0.97      0.96      0.96     11542\n",
            "        Male       0.95      0.96      0.95      8459\n",
            "\n",
            "    accuracy                           0.96     20001\n",
            "   macro avg       0.96      0.96      0.96     20001\n",
            "weighted avg       0.96      0.96      0.96     20001\n",
            "\n"
          ]
        }
      ]
    }
  ]
}